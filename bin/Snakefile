#! /usr/bin/env bash
## Snakefile
####################
#from os import listdir
#print(listdir())
import glob
import re

BASE_DIR = workflow.basedir + "/../"

## find the samples in the fastq directory. 
fastqs = glob.glob("fastq/*.fastq*")
SAMPLES = [re.sub("fastq/(.+).fastq.*$","\\1",a) for a in fastqs]
FASTQ_DICT = dict(zip(SAMPLES,fastqs))
print("Found samples:")
print(SAMPLES)
## dependent programs
MARKDUP=BASE_DIR+"dependencies/picard.jar MarkDuplicates"
PHANTOMPEAKTOOL = BASE_DIR + "dependencies/run_spp_nodups.R"
WIG2BIGWIG=BASE_DIR + "dependencies/wigToBigWig"

## global variables
GENOME = config["GENOME"]
BWA_INDEX_NAME = config["BWA_INDEX_PATH"]+GENOME+ ".fa"
#BWA_INDEX_NAME = "/mnt/silencer2/share/bwa_indices/"+GENOME+".fa"
MAPQ_THRESH = "0"
NREADS= "15000000"

rule vanilla:
    input: 
        expand("bigWig/{sample}.nodup.bw", sample=SAMPLES),
        expand("bam/{sample}.filt.nodup.srt.bam", sample=SAMPLES),
        expand("qc/{sample}.filt.nodup.sample."+str(int(NREADS)/1000000)+".SE.tagAlign.gz.cc.qc",sample=SAMPLES),
        "all_sample.qc.txt"

rule bwa_map:
    input: 
        lambda wildcards: FASTQ_DICT[wildcards.sample]
        #"fastq/{sample}.fastq.bz2"
    output:
        raw = temp ("bam/{sample}.raw.srt.bam"),
        filt = temp("bam/{sample}.filt.srt.bam"),
        sai = temp("bam/{sample}.raw.sai"),
        raw_qc = "qc/{sample}.raw.flagstat.qc",
        filt_qc = "qc/{sample}.filt.flagstat.qc"
    log: 
        "logs/bwa_map/{sample}.log"
    threads: 6
    shell:
       # "bwa aln -q 5 -l 32 -k 2 -t {threads} {BWA_INDEX_NAME} <( bzcat {input} )"
       # " > {output.sai} 2> {log};"
       # "bwa samse {BWA_INDEX_NAME} {output.sai} <(bzcat {input}) 2>> {log}|"
        "bash {BASE_DIR}/bin/bwa_aln_fmt_autodetect.sh {threads} {BWA_INDEX_NAME} {input}"
        " {output.sai} {log}|"
        "samtools view -@ {threads} -Su - |"
        "samtools sort -@ {threads} -m 4G - -T {wildcards.sample} -o {output.raw};"
        "samtools flagstat {output.raw} > {output.raw_qc};"
        "samtools view -@ {threads} -F 1804 -q {MAPQ_THRESH} -b {output.raw} > {output.filt};"
        "samtools flagstat {output.filt} > {output.filt_qc}"

rule bam_markdup:
    input:
        bam = "bam/{sample}.filt.srt.bam",
        qc = "qc/{sample}.filt.flagstat.qc"
    output:
        bam = temp("bam/{sample}.dupmark.bam"),
        qc = "qc/{sample}.dup.flagstat.qc"
    log: 
        "logs/markdup/{sample}.markdup.log"
    threads: 3
    shell: 
        "java -Xmx12G -XX:ParallelGCThreads=3 -jar {MARKDUP} TMP_DIR=tmp/{wildcards.sample} INPUT={input.bam} OUTPUT={output.bam} METRICS_FILE={output.qc} VALIDATION_STRINGENCY=LENIENT ASSUME_SORTED=true REMOVE_DUPLICATES=false 2> {log}"

rule bam_finalize:
    input:
        bam = "bam/{sample}.dupmark.bam",
        qc = "qc/{sample}.dup.flagstat.qc"
    output:
        bam = "bam/{sample}.filt.nodup.srt.bam",
        stat = "qc/{sample}.filt.nodup.srt.flagstat.qc",
        pbc = "qc/{sample}.filt.nodup.srt.pbc.qc"
    threads: 1
    shell:
        "samtools view -F 1804 -b {input.bam} > {output.bam};"
        "samtools index {output.bam};"
        "samtools flagstat {output.bam} > {output.stat};"
        "bedtools bamtobed -i {input.bam} |"
        "awk 'BEGIN{{OFS=\"\\t\"}}{{print $1,$2,$3,$6}}' |"
        "grep -v 'chrM' | sort -T tmp/{wildcards.sample} | uniq -c |"
        "awk 'BEGIN{{mt=0;m0=0;m1=0;m2=0}} ($1==1){{m1=m1+1}} ($1==2){{m2=m2+1}}"
        "{{m0=m0+1}}{{mt=mt+$1}} END{{printf\"%d\\t%d\\t%d\\t%d\\t%f\\t%f\\t%f\\n\","
        "mt,m0,m1,m2,m0/mt,m1/m0,m1/m2}}' > {output.pbc}"

rule phantom:
    input:
        bam = "bam/{sample}.filt.nodup.srt.bam"
    output: 
        tag_align = temp("{sample}.filt.nodup.srt.SE.tagAlign.gz"),
        subsample = temp("{sample}.filt.nodup.sample."+str(int(NREADS)/1000000)+".SE.tagAlign.gz"),
        plot = "qc/{sample}.filt.nodup.sample."+str(int(NREADS)/1000000)+".SE.tagAlign.gz.cc.plot.pdf",
        cc = "qc/{sample}.filt.nodup.sample."+str(int(NREADS)/1000000)+".SE.tagAlign.gz.cc.qc",
        cc_temp = temp("qc/{sample}.cc.temp")
    log: 
        "logs/phantompeak/{sample}.phantom.log"
    threads: 1
    shell: 
        "bedtools bamtobed -i {input.bam} | awk 'BEGIN{{OFS=\"\\t\"}}{{$4=\"N\";$5=\"1000\";print $0}}' | gzip -c > {output.tag_align};"
        "zcat {output.tag_align} | grep -v \"chrM\" | shuf -n {NREADS} |"
        "gzip -c > {output.subsample};"
        "Rscript {PHANTOMPEAKTOOL} -c={output.subsample} -filtchr=chrM "
        "-savp={output.plot} -out={output.cc} -tmpdir=tmp/{wildcards.sample} -rf &> {log};"
        "sed -r 's/,[^\\t]+//g' {output.cc} > {output.cc_temp};"
        "cp {output.cc_temp} {output.cc}"

rule bam2bw:
    input: 
        "bam/{sample}.filt.nodup.srt.bam"
    output:
        bw = "bigWig/{sample}.nodup.bw"
    threads: 4
    shell:
      "bamCoverage -b {input} -o {output.bw} --outFileFormat bigwig "
      "-bs 50 --numberOfProcessors {threads} --normalizeUsing RPKM"


#rule clean: 
#    """ cleaning up temperory files """

rule lastqc:
    input:
        raw = expand("qc/{samples}.raw.flagstat.qc",samples=SAMPLES),
        dup = expand("qc/{samples}.dup.flagstat.qc",samples=SAMPLES)
    output:
        "{sample}.qc.txt"
    threads: 1
    run:
      out = open(output[0],'w')
      out.write("\t".join(["Sample","Total","Mapped","Filtered","Uniq","Map%","Dup%"])+"\n")
      for idx in range(len(input.raw)):
        samples = re.match(r"qc\/(.*).raw.flagstat.qc",input.raw[idx]).groups()[0]
        raw_file = open(input.raw[idx], 'r')
        for line in raw_file:
          words = line.strip().split(' ')
          if len(words)>5:
            if words[4] == "total":
              total = words[0]
            elif words[3] == "mapped":
              mapped = words[0]
        raw_file.close()
        dup_file = open(input.dup[idx],'r')
        for line in dup_file:
          words = line.strip().split('\t')
          if words[0] == "Unknown Library":
            filt = words[1]
            duplicates = words[5]
            nodup = str(int(filt)-int(duplicates))
        map_p = "%.2f"%(float(mapped)/float(total))
        dup_p = "%.2f"%(float(duplicates)/float(mapped))
        out.write("\t".join([samples,total,mapped,filt,nodup,map_p,dup_p])+"\n")

        
